---
title: "Module: Evaluating Sources"
slug: "Module-Evaluating-Sources"
author: "SAUFEX Consortium"
date: "2026-01-23"
description: "Practical approaches to source evaluation - what signals matter, what doesn't, and how to make decisions under uncertainty."
learningPath: "Media Literacy Fundamentals"
moduleNumber: 1
estimatedTime: "16 minutes"
---
**Purpose**: You'll learn to assess source credibility quickly and accurately, distinguishing strong signals from weak ones, without falling into common evaluation traps.

---

[screen 1]

## The Problem

An article appears in your feed. The headline is alarming. The claim is specific. Should you trust it? Share it? Investigate further?

You don't have unlimited time. You can't verify everything. But you can be systematic.

This module provides a framework — not a checklist that "catches all lies," but a practical approach to source assessment under real-world constraints.

---

[screen 2]

## What Source Evaluation Isn't

Let's clear some misconceptions:

**Not a truth machine**
Source evaluation tells you about reliability, not about whether specific claims are true.

**Not binary**
Sources aren't simply "credible" or "not credible." They're more or less reliable for specific purposes.

**Not foolproof**
Good sources sometimes get things wrong. Bad sources occasionally get things right.

The goal is better probability assessment, not certainty.

---

[screen 3]

## The Evidence Ladder for Sources

Classify source signals by strength:

**Strong signals:**
- Documented track record of accuracy
- Clear editorial standards with accountability
- Named authors with verifiable expertise
- Transparent correction policies (and use of them)
- Institutional reputation over time

**Medium signals:**
- Professional presentation
- Clear organizational identity
- Some credentials visible
- Mix of accurate and inaccurate past content

**Weak signals:**
- Domain name sounds authoritative
- Professional website design
- The content confirms what you believe
- Others are sharing it

Weak signals are noise. Strong signals are information.

---

[screen 4]

## Starting Point: Lateral Reading

Experts don't evaluate sources by staring at them. They read laterally — leaving the site to see what others say about it.

**The technique:**
1. Open a new tab
2. Search for the source name + "credibility" or "reliability"
3. Check what Wikipedia says (not as final word, but as starting point)
4. Look for coverage in sources you already trust
5. Check fact-checker assessments if available

This takes 30 seconds and provides more signal than 10 minutes studying the source itself.

---

[screen 5]

## Authority: The Expertise Question

"Who created this?" matters — but expertise is specific.

**What to assess:**
- Does the author have relevant credentials?
- Do credentials match the subject matter?
- Is there a track record on this topic?
- Would peers in the field recognize this person?

**Common traps:**
- A PhD in one field isn't expertise in another
- "Institute" or "Foundation" in a name proves nothing
- Popular following doesn't equal expertise
- Credentials can be fabricated (verify if stakes are high)

Authority is domain-specific. A cardiologist speaking about cardiology is authoritative. The same cardiologist speaking about climate science is a layperson.

---

[screen 6]

## The Credentials Verification Problem

How hard should you work to verify credentials?

**For casual browsing:**
- Quick lateral reading is sufficient
- Check if institution exists
- Note claims, don't over-invest

**For sharing/acting:**
- Verify author actually works where claimed
- Check if credentials are from legitimate institutions
- Look for professional recognition

**For high-stakes decisions:**
- Full verification of credentials
- Check for retractions or corrections
- Contact institution if necessary

Match verification effort to stakes. Not everything requires investigation.

---

[screen 7]

## Accuracy: Evidence Quality

Reliable sources show their work:

**Strong accuracy signals:**
- Links to primary sources
- Specific citations (not "studies show")
- Original documents or data
- Quotes that can be traced
- Acknowledgment of limitations

**Weak accuracy signals:**
- Vague references ("experts say")
- Screenshots instead of links (unverifiable)
- Circular sourcing (sources citing each other)
- Missing dates on data
- Claims that require trust without evidence

The question isn't "Does this have citations?" It's "Could I trace these citations if I needed to?"

---

[screen 8]

## The SIFT Method

A practical framework that works:

**S — Stop**
Don't share, don't engage, don't react until you've assessed.

**I — Investigate the source**
Lateral reading: what do others say about this source?

**F — Find better coverage**
Is this claim covered elsewhere? By more reliable sources?

**T — Trace claims**
What's the original source? Does the claim hold up when you trace it?

SIFT takes 60-90 seconds. It catches most problems.

---

[screen 9]

## Agenda: Understanding Purpose

Every source has a purpose. Knowing it helps calibration.

**Purpose types:**
- **Inform**: Journalism, education, reference
- **Sell**: Advertising, marketing, affiliate content
- **Persuade**: Advocacy, opinion, political communication
- **Entertain**: Satire, parody, clickbait
- **Manipulate**: Disinformation, scams, propaganda

**Key insight:**
Agenda doesn't automatically mean unreliable. Advocacy groups can be factually accurate. News can be biased. The question is: does the agenda compromise accuracy?

---

[screen 10]

## Practical Scenario

**Situation**: An article claims a new study proves a controversial health treatment is effective. The article is shared by someone you trust. The source is unfamiliar.

**Time available**: 3 minutes

**Assessment task**: Should you share, investigate further, or pass?

---

[screen 11]

## Working the Scenario

**Step 1: Lateral reading (30 sec)**
- Search source name
- Check if it's a known health information site
- Look for fact-checker assessments

**Step 2: Author check (30 sec)**
- Named author with credentials?
- Can you verify the credentials exist?

**Step 3: Evidence check (60 sec)**
- Does article link to actual study?
- Is study published in peer-reviewed journal?
- Does the article's claim match what study actually says?

**Step 4: Decision (60 sec)**
- Strong signals → share with source noted
- Medium signals → investigate further or don't share
- Weak signals only → don't share, don't amplify

---

[screen 12]

## Sample Assessment

"Article claims 'New study proves Treatment X works.' Source is healthnewsnetwork.info.

**Lateral reading**: Site not found in major source databases. Domain registered 6 months ago. No Wikipedia entry.

**Author**: Byline says 'Health Desk' — not a named person. No author credentials.

**Evidence**: Article doesn't link to study. Says "published in a leading journal" without naming it. No DOI or citation.

**Assessment**: Weak-to-medium signals only. Source unestablished, no verifiable author, evidence untraced.

**Decision**: Do not share. If topic matters to me, find better coverage from established health reporting.

**Confidence**: Medium. Could be legitimate new source with poor practices, but no basis to trust it."

---

[screen 13]

## Common Evaluation Errors

**Error 1: Domain authority confusion**
".org" doesn't mean nonprofit. ".edu" can host student blogs. Domain tells you nothing about reliability.

**Error 2: Design equals credibility**
Professional websites are cheap. Scam sites often look better than legitimate ones.

**Error 3: Confirmation = credibility**
If you want something to be true, you'll find the source more credible. This is bias, not evaluation.

**Error 4: Popular = reliable**
Viral content and follower counts are not accuracy indicators.

**Error 5: Complexity = authority**
Jargon and detailed-looking content can mask nonsense. Actual experts often explain simply.

---

[screen 14]

## The Publication Landscape

Different source types have different reliability patterns:

**Wire services (AP, Reuters, AFP)**
- Generally high reliability for facts
- Errors happen but corrections are standard
- Not analytical — just reporting

**Major newspapers**
- Variable quality by section (news vs. opinion)
- Track record matters
- Corrections policies visible

**Specialist publications**
- High reliability within domain
- May lack perspective outside domain
- Credentialing often rigorous

**Partisan media**
- Facts may be accurate but selective
- Framing reflects agenda
- Reliability varies widely

**Independent creators**
- Extremely variable
- Some excellent, some terrible
- Track record is everything

---

[screen 15]

## When to Stop Evaluating

Source evaluation can become a time sink. Know when you've done enough:

**Stop when:**
- Lateral reading shows source is established and reliable
- Multiple strong signals confirm credibility
- Source matches your existing knowledge of reliable sources
- Time invested exceeds decision stakes

**Keep going when:**
- Stakes are high (health, safety, major decisions)
- Signals are mixed or contradictory
- Source is new to you and you plan to rely on it
- Claims are extraordinary

Proportionality matters. Not every source requires investigation.

---

[screen 16]

## What To Do With Your Assessment

Source evaluation leads to action:

**High reliability:**
- Use the information with appropriate confidence
- Note the source if sharing
- Still apply normal critical thinking

**Medium reliability:**
- Look for corroboration before relying on it
- Note uncertainty if sharing
- Don't cite as definitive

**Low reliability:**
- Don't share
- Don't use for decisions
- May still investigate if topic matters

The assessment shapes how you use information, not just whether you "trust" it.

---

[screen 17]

## Building Source Literacy Over Time

Expertise develops:

**Short-term:**
- Use SIFT for unfamiliar sources
- Build a mental map of reliable sources in topics you follow
- Notice when you're skipping evaluation because you want to believe

**Long-term:**
- Source recognition becomes automatic for familiar domains
- Lateral reading gets faster as you know where to look
- You develop instinct for what "feels off" — but you verify instinct with evidence

This is a skill. It improves with practice.

---

[screen 18]

## Module Assessment

**Scenario**: You encounter three sources covering the same news event:

**Source A**: Major newspaper with known editorial standards, article by named reporter, links to primary documents

**Source B**: Website with "news" in domain, no named author, claims to have "exclusive" documents but doesn't show them

**Source C**: Tweet thread by account with 50,000 followers, claims to be an expert, no institutional affiliation visible

**Task (8 minutes):**

1. Rate each source's signal strength (strong/medium/weak)
2. What one thing would you check for each to increase confidence?
3. If you could only use one source, which and why?
4. What would you NOT claim based on Source B alone?

**Scoring:**
- Credit distinguishing signal strength
- Reward proportionate verification suggestions
- Penalize treating follower count as credibility

---

[screen 19]

## Key Takeaways

- Source evaluation provides probability assessment, not truth verification
- Strong signals: track record, transparent standards, verifiable expertise, correction policies
- Weak signals: domain names, design, popularity, confirmation of your beliefs
- Lateral reading beats staring at the source — check what others say
- SIFT: Stop, Investigate source, Find better coverage, Trace claims
- Authority is domain-specific; credentials must match subject
- Agenda doesn't equal unreliable, but know the purpose
- Match verification effort to stakes
- Don't share on weak signals; corroborate on medium signals
- This is a skill that improves with practice

---

## Next Module

Continue to: **Identifying Bias** — Types of bias, how to recognize them, and how to account for bias without dismissing sources entirely.
